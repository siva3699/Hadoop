##############-- data loading using spark sql --################


1.Json file


val studentDF = sqlContext.read.format("json").load("file:///root/baby.json")



2. CSV file


 Include spark-csv as an sbt dependency
 
 "com.databricks" % "spark-csv_2.10" % "1.3.0",
 
 
val baby_namesDF = sqlContext.read.format("com.databricks.spark.csv")
.option("header", "true")
.option("inferSchema", "true")
.load("baby_names.csv")



// In the above code, we are specifying the desire to use com.databricks.spark.csv format from the package we passed to the shell in step 1. 
“header” set to true signifies the first row has column names. 
“inferSchema” instructs Spark to attempt to infer the schema of the CSV and finally load function passes in the path and name of the CSV source file.//



baby_namesDF.show()


//Register a temp table

 
scala> baby_namesDF.registerTempTable("names")



3. parquet

studentDF.write.parquet("file:///root/people.parquet")


val studentparDF = sqlContext.read.format("parquet").load("file:///root/people.parquet")



val peopleDF = sqlContext.read.format("parquet").load("file:///examples/src/main/resources/people.parquet")



4.textfile


val peopleDF = sqlContext.read.format("text").load("file:///examples/src/main/resources/people.text")
















