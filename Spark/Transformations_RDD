1. map(func)

###Return a new distributed dataset formed by passing each element of the source through a function func ####


Spark map Example Using Scala

// Basic map example in scala
scala> val x = sc.parallelize(List("spark", "rdd", "example",  "sample", "example"), 3)
scala> val y = x.map(x => (x, 1))
scala> y.collect
res0: Array[(String, Int)] = Array((spark,1), (rdd,1), (example,1), (sample,1), (example,1))

##with the map transformation we are mapping each element with integer 1 and creating a tuple like (word, 1) ##

// rdd y can be re writen with shorter syntax in scala as 
scala> val y = x.map((_, 1)) 
scala> y.collect
res0: Array[(String, Int)] = Array((spark,1), (rdd,1), (example,1), (sample,1), (example,1))

// Another example of making tuple with string and it's length
scala> val y = x.map(x => (x, x.length))
scala> y.collect
res0: Array[(String, Int)] = Array((spark,5), (rdd,3), (example,7), (sample,6), (example,7))



2. flatMap(func)

Similar to map, it returns a new RDD by applying  a function to each element of the RDD, but output is flattened.
Also, function in flatMap can return a list of elements (0 or more)

Let’s see this with an example.

Say you have a text file as follows

Hello World
Who are you

Now, if you run a flatMap on the textFile rdd,

words = linesRDD.flatMap(x -> List(x.split(“ “)))
And, the value in the words RDD would be, [“Hello”, “World”, “Who”, “are”, “you”]




3. filter(func)

##filter(func) returns a new data set (RDD) that's formed by selecting those elements of the source on which the function returns true.##

ex:- 

Spark filter Example Using Scala

scala> val x = sc.parallelize(1 to 10, 2)

// filter operation 
scala> val y = x.filter(e => e%2==0) 
scala> y.collect
res0: Array[Int] = Array(2, 4, 6, 8, 10)

// rdd y can be re written with shorter syntax in scala as 
scala> val y = x.filter(_ % 2 == 0)
scala> y.collect
res1: Array[Int] = Array(2, 4, 6, 8, 10)

As you can see in above example RDD X is the source RDD and contains elements 1 to 5 and has two partitions. 
Operation filter is take predicate f(x) as an argument which is some thing like x % 2 == 0
it means it will return true for even elements and false for odd elements. 
RDD Y is a resulting RDD which will have the filtered (i.e. even elements).

filter(x => x.isEmpty)

#### to filter the empty strings from RDD ###

filter(y => y.nonEmpty)

#### to filter the non empty strings from RDD ###

filter(z => z.contains("string to search"))

### to filter RDD's that contains a particular string. its simillar to like in SQL with %string% #### 

filter(w => !w.contains("string to search"))

### to filter RDD's  that does not contains a particular string. its simillar to not like in SQL with %string% #### 




4. sortByKey([ascending], [numTasks])

When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, 
as specified in the boolean ascending argument.

myrdd.sortByKey(true)

true => ascending
false => descending


Example 1:Take above example to sort by key asc or desc.
scala> kv.sortByKey(true).collect
res35: Array[(String, Int)] = Array((apple,2), (apple,1), (banana,3), (orange,2))

scala> kv.sortByKey(false).collect
res36: Array[(String, Int)] = Array((orange,2), (banana,3), (apple,2), (apple,1))


5. distinct([numTasks]))   

Return a new dataset that contains the distinct elements of the source dataset.

Example 1: Return distinct values from one array.
scala> val list  = sc.parallelize(List("apple", "orange", "banana", "apple", "orange"))
scala> list.distinct.collect
res13: Array[String] = Array(orange, apple, banana)



6. coalesce(numPartitions)

Decrease the number of partitions in the RDD to numPartitions. 
Useful for running operations more efficiently after filtering down a large dataset.

Keep in mind that repartitioning your data is a fairly expensive operation. 
Spark also has an optimized version of repartition() called coalesce() that allows avoiding data movement, 
but only if you are decreasing the number of RDD partitions.

One difference I get is that with repartition() the number of partitions can be increased/decreased, 
but with coalesce() the number of partitions can only be decreased.

If the partitions are spread across multiple machines and coalesce() is run, how can it avoid data movement?


It avoids a full shuffle. If it's known that the number is decreasing then the executor can safely keep data on the minimum number of partitions, 
only moving the data off the extra nodes, onto the nodes that we kept.

So, it would go something like this:

Node 1 = 1,2,3
Node 2 = 4,5,6
Node 3 = 7,8,9
Node 4 = 10,11,12
Then coalesce down to 2 partitions:

Node 1 = 1,2,3 + (10,11,12)
Node 3 = 7,8,9 + (4,5,6)
Notice that Node 1 and Node 3 did not require its original data to move.




coalesce will just combine original partitions to the new number of partitions. 
In that sense, coalesce will only reduce the number of partitions.

Since shuffling could be very costly, if reduce the number of partitions is what you really want to do, 
please consider use coalesce.





7. aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])




/* aggregateByKey takes an initial accumulator (here an empty list),
   a first lambda function to merge a value to an accumulator, and a
   second lambda function to merge two accumulators */

      RDD.aggregateByKey(List[Int]())(
        (aggr, value) => value + aggr,
        (aggr1, aggr2) => aggr1 + aggr2
      ).foreach(println)
      
      
      
      Ex: 1) - 
      
      
      
      scala> val nameCounts = sc.parallelize(List(("David", 6), ("Abby", 4), ("David", 5), ("Abby", 5)))
             nameCounts: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:12
             
             
             scala> nameCounts.aggregateByKey(0)((accum, v) => accum + v, (v1, v2) => v1 + v2).collect
                    res1: Array[(String, Int)] = Array((Abby,9), (David,11))
      
      
    
